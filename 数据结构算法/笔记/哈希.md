《极客时间》中各数据结构的概念要计上来


# 哈希函数
    1.输入域无穷大
    2.输出域有穷尽
    3.输入一样，输出值一定不变
    4.输入不一样，有可能得到一样的值（哈希冲撞）
    5.离散型：输入足够多时，输入会在域上均匀分布 
    6.一个哈希函数可以组合出n种哈希函数
    7.应用
        1.加密
        2.唯一标识 一张图的二进制码串开头取100个字节 中间取100个字节 最后再取100个字节
        3.数据校验 迅雷下载文件是P2P 如何保证多个机器上的文件块没有被篡改过
        4.散列函数  存储密码时加salt 防止被猜出来
        5.负载均衡  实现同一个IP的请求都路由到同一个服务器上
        6.数据分片  
                    1.先对数据分片，然后采用多台机器处理的方法，来提高处理速度。
                    有1000个机器，把每行字符串对1000取模，得到被分配的机器编号。
                    每台机器再计算本机上有的字符串数量，就是重复的次数。
                    应用：在大容量日志文件中找出每个关键词被重复搜索的次数
                    2.判断图片是否在图库中
                    n台服务器 每张图片计算唯一标识后与n取模 得到要分配的机器编号
        7.分布式存储 
            1.一致性哈希

    

# 哈希表
    1.增删改查是O(1)、
    2.如何扩容
        当装载因子大于一个阈值时，需要扩容。如果一次性扩容，需要把旧表中所有数据重新计算hash值。
        解决办法：
            当装载因子到达阈值时，不搬运。有新数据要插入时，插到新表中，同时在旧表中拿出一个数据
            插入新表。
            此期间的查询操作：现在新表中查，没有就去旧表中查。
    3.JVM中，每个桶连的结构是红黑树
    4.hashmap的put方法会覆盖
    5.装载因子 = 填入表中的元素个数 / 散列表的长度
    6.散列攻击 精心构造数据 使得数据散列到同一个槽里面 退化成链表
    7.解决散列冲突：
        1.开放寻址法：
            1.java中threadlocalmap就是线性探测的开放寻址法来解决
            2.数据都存储在数组中，有效的利用CPU缓存加快查询速度 序列化简单
            3.当装在因子小，数据量小时，适合开放寻址法
        2.链表法
            1.插入时间复杂度O(1)
            2.查找 删除一个元素 O(K)  k = n/m n为数据个数 m为散列表槽数
            3.需要创建链表，内存消耗太大
            4.可用红黑树 跳表等结构代替链表
    8.散列表与双向链表结合 redis 散列表与跳表结合
    9、linkedhashmap  linked指的是双向链表 按照访问时间排序 本身就是一个LRU缓存淘汰策略的缓存系统

[问题]：有一个大文件100T，每一行是一个字符串，打印重复的字符串。
    思路：与MapReduce一样。
    难点    
        1.数据量太大 不能放到内存中计算
        2.只用一台机器，时间太长
    解法：
        先对数据分片，然后采用多台机器处理的方法，来提高处理速度。
        有1000个机器，把每行字符串对1000取模，得到被分配的机器编号。
        每台机器再计算本机上有的字符串数量，就是重复的次数。
    应用：在大容量日志文件中找出每个关键词被重复搜索的次数

[问题1] 设计RandomPool结构，有如下三个功能：
        insert(key),将某个Key加入到该结构中，做到不重复加入
        delete(key),将某个key移除
        getRandom(),严格等概率随机返回结构中任何一个key    要求时间复杂度都为O(1)

    思路：
        1.准备两张hash表，才能等概率返回。一个哈系表的分布并不是严格等概率的。准备一个size变量
        第0个进来得是A，map1中存入(A,0) map2中存入(0,A) size计数 然后再math.random中随机选size中
        一个数字，直接在map2中查找。这样返回的是严格的等概率
        2. 要保证map中的Integer是0~size-1 所以每移除一个key,就把最大的index
        对应的key放到被移除的位置
    思考：为什么不直接用一个数组或者哈希表存，因为这样需要遍历整个数组，而哈希表一般不用value找key。

# 布隆过滤器 [搜索相关的公司都会问]
    需求：100亿个URL黑名单，每个链接64字节，用户输入url,在黑名单返回false，不在返回true。
    所以用hashset，至少需要6400亿字节(640G)。
    *** 用分布式的话需要多台电脑，如何使用一台电脑实现 ***
    缺点：布隆过滤器有失误率，有些网站不在黑名单也会返回false。(宁可错杀，不会放过)
    面试过程：先说分布式，面试官说浪费内存。再问允许失误率吗，面试官说可以。再讲布隆过滤器。
    实现：
        1.bit类型的map
        准备一个int数组[0~m-1]，数组里面每个位置上是一个Bit(0或1)
        一个int是4字节32位bit 1000个int能表示32000个bit 若要把第Index个bit变为1
        用index/和%32得到bitindex，即在该32位bit中的index， 再arr[index]|(1<<bitindex)
        把该位置的bit变为1
        想省空间 可以把int变为long 
        2.已有一个bitmap,长度为m(此处m和1中m不一样)。url经过k个哈希函数算出k个hashcode，每个hashcode再%m，得到k个数，
        把bitmap中k个位全置为1。
        3.等判断时，将输入对象经过这k个哈希函数计算得到k个值，然后判断对应bitarray的k个位置是否都为1（是否标黑），如果有一个不为黑，那么这个输入对象则不在这个集合中，也就不是黑名单了！如果都是黑，那说明在集合中，但有可能会误，由于当输入对象过多，而集合也就是bitarray过小，则会出现大部分为黑的情况，那样就容易发生误判
        4.bitmap的大小和样本量、失误率有关。m = -n*lnP/(ln2)^2  n为样本量 P为失误率
        5.n=100亿，P=0.0001时，m=1700亿bit，约223亿字节，约22.3G 可以适量调大 会降低失误率
        6.确定哈希函数的个数 K=ln2*(m/n) 
        7.m,k向上取整之后，真实失误率P=(1-e^(-n*k/m))^k



# 一致性哈希
    服务器经典抗压结构：字符串经哈希处理后%上n(n为服务器个数)得到服务器编号并放入，这样负载均衡。
    缺点是扩容时需要全部重算。
    一致性哈希：把数据迁徙的代价变低。字符串哈希之后不%了。



